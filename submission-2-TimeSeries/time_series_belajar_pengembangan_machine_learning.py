# -*- coding: utf-8 -*-
"""Time_Series_Belajar_Pengembangan_Machine_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uRbt5aDQTXdB5D5ozRVsn8rfan5JclaD

# Submisi Time Series Belajar Pengembangan Machine Learning
# Predict total revenue
## Nama : Husni Naufal Zuhdi

## Scoring Criterias

Minimum Criteria
1. Dataset yang akan dipakai bebas, namun minimal memiliki 1000 sampel.(v)
2. Harus menggunakan LSTM dalam arsitektur model.(v)
3. Validation set sebesar 20% dari total dataset.(v)
4. Model harus menggunakan model sequential.(v)
5. Harus menggunakan Learning Rate pada Optimizer.(v)
6. MAE < 10% skala data.(v)

High Score Criteria
1. Dataset yang digunakan memiliki banyak sampel data.(v)
2. Mengimplementasikan Callback.(v)
3. Membuat plot loss dan akurasi pada saat training dan validation.(v)

4 Star Criteria
1. Semua ketentuan terpenuhi.(v)
2. Dataset memiliki minimal 2000 sampel data.(v)
3. MAE dari model < 10% skala data.(v)

5 Star Criteria
1. Semua ketentuan terpenuhi.(v)
2. Dataset memiliki minimal 10000 sampel data.(x)
3. MAE dari model < 10% skala data.(v)

## Pre Processing
"""

# The process to acces Kaggle API refer to this article
# https://www.kaggle.com/general/74235
# Install kaggle API
! pip install -q kaggle

# Insert my kaggle json key
from google.colab import files
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

# # Download E-Commerce Data
# # https://www.kaggle.com/carrie1/ecommerce-data
! kaggle datasets download -d carrie1/ecommerce-data

# Unzip the dataset
! unzip /content/ecommerce-data.zip

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split
from keras.layers import Dense, LSTM

# Read train csv file
df = pd.read_csv('/content/data.csv', encoding= 'unicode_escape')
df.head()

# Gain insights
df.info()

# Check Null Value
df.isnull().sum()

# Pre processing data.csv
# Convert Invoice Date data type into date type
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], dayfirst=True)
df.sort_values('InvoiceDate', inplace=True)

# Combine Quantity * Unit Price to get Total Revenue/purchase
df['TotalRevenue'] = df['Quantity'] * df['UnitPrice']

# Agregate Total Revenue/purchase into Total Revenue/day
df = df.groupby('InvoiceDate')[['Quantity', 'TotalRevenue']] \
                .sum().reset_index()
df.head()

# Transform dataset period to hourly
hourly_sales = df.resample('1H', on = 'InvoiceDate').sum().reset_index()
hourly_sales.head()

# Check Null Value
hourly_sales.isnull().sum()

# Check dataset info
hourly_sales.info()

# Convert dataset into numpy array
dates = hourly_sales['InvoiceDate'].values
quantity = hourly_sales['Quantity'].values
tot_rev  = hourly_sales['TotalRevenue'].values

# Plot sales quantity
plt.figure(figsize=(15,5))
plt.plot(dates, quantity)
plt.title('Sales Quantity',fontsize=20);

# Plot total revenue
plt.figure(figsize=(15,5))
plt.plot(dates, tot_rev)
plt.title('Total Revenue',fontsize=20);

"""Because the dataset does not contain much data in the frist year, we need to ampute the first year data an continue use the second year data.

Ampute the data from row 0 to row 8000

24(hour/day)*365(day/year)*1(year) = 8760 or approximate 8000
"""

# Ampute the first 8500 rows
hourly_sales_2 = hourly_sales.iloc[8000:,:]
# Convert dataset into numpy array
dates_2 = hourly_sales_2['InvoiceDate'].values
quantity_2 = hourly_sales_2['Quantity'].values
quantity_2 = tf.cast(quantity_2, dtype='float64')
tot_rev_2  = hourly_sales_2['TotalRevenue'].values
tot_rev_2 = tf.cast(tot_rev_2, dtype='float64')

# Check the dataset info
hourly_sales_2.info()

# Plot sales quantity
plt.figure(figsize=(15,5))
plt.plot(dates_2, quantity_2)
plt.title('Sales Quantity in Second Year',fontsize=20);

# Plot total revenue
plt.figure(figsize=(15,5))
plt.plot(dates_2, tot_rev_2)
plt.title('Total Revenue in Second Year',fontsize=20);

"""This is better"""

# Split data into train and test data
dates_train, dates_test= np.split(dates_2, [int(.80 *len(dates_2))])
quantity_train, quantity_test= np.split(quantity_2, [int(.80 *len(quantity_2))])
tot_rev_train, tot_rev_test= np.split(tot_rev_2, [int(.80 *len(tot_rev_2))])

"""## Build Machine Learning Model"""

# Build windowed dataset function
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

# Make Train and Test sets
quantity_train_set = windowed_dataset(quantity_train,
                                      window_size=60,
                                      batch_size=100,
                                      shuffle_buffer=1000)
quantity_test_set = windowed_dataset(quantity_test,
                                      window_size=60,
                                      batch_size=100,
                                      shuffle_buffer=1000)
revenue_train_set = windowed_dataset(tot_rev_train,
                                      window_size=60,
                                      batch_size=100,
                                      shuffle_buffer=1000)
revenue_test_set = windowed_dataset(tot_rev_test,
                                      window_size=60,
                                      batch_size=100,
                                      shuffle_buffer=1000)

# Build our Model
model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.LSTM(128),
  tf.keras.layers.Dense(128, activation="relu"),
  tf.keras.layers.Dense(64, activation="relu"),
  tf.keras.layers.Dense(1),
])
optimizer = tf.keras.optimizers.SGD(learning_rate=0.0005, momentum=0.5)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

# Check the description of TotalRevenue Column to get max and min values
hourly_sales_2['TotalRevenue'].describe()

# Check the description of Quantity Column to get max and min values
hourly_sales_2['Quantity'].describe()

"""To Calculate Scale of Data we need max value and min value from our dataset

1. TotalRevenue

* (max-min) = 54089.630000 - (-32651.680000) = 86741.31
* (max-min) = ABS(54089.630000) - ABS(-32651.680000) = 21437.95

We will only use TotalRevenue feature because the Quantity feature data scale have negative value (I like to play it safe :) )

Criteraia given by dicoding are we need to get MAE score bellow 10% of Data Scale

* 0.1*86741.31 = 8674.131
* 0.1*21437.95 = 2143.795
"""

# Build Callback Class
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<500):
      if(logs.get('val_mae')<500):
        print("\nModel Mean Absolute Error and Validation Mean Absolute Error bellow 500!")
        self.model.stop_training = True
callbacks = myCallback()

"""## Train and Test Model"""

# Test our Machine Learning Model to predict Total Revenue
num_epochs = 200
history = model.fit(revenue_train_set,
                    validation_data=revenue_test_set,
                    epochs=num_epochs,
                    callbacks=[callbacks])

# See model summary
model.summary()

"""## Evaluate"""

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Model Mean Absolute Error')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

