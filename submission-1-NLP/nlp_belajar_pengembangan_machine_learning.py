# -*- coding: utf-8 -*-
"""NLP_Belajar_Pengembangan_Machine_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C7N0oWNORTvXqBKJRyWvRLxs7UJj8F4j

# Submisi NLP Belajar Pengembangan Machine Learning
## Nama : Husni Naufal Zuhdi

## Scoring Criterias

Minimum Criteria
1. Dataset yang akan dipakai bebas, namun minimal memiliki 1000 sampel. (v)
2. Harus menggunakan LSTM dalam arsitektur model. (v)
3. Harus menggunakan model sequential. (v)
4. Validation set sebesar 20% dari total dataset. (v)
5. Harus menggunakan Embedding. (v)
6. Harus menggunakan fungsi tokenizer. (v)
7. Akurasi dari model minimal 75%. (v)

High Score Criteria
1. Akurasi dari model di atas 80%. (x)
2. Mengimplementasikan callback. (v)
3. Membuat plot loss dan akurasi pada saat training dan validation. (v)

4 Star Criteria
1. Semua ketentuan terpenuhi (?)
2. Dataset memiliki minimal 2000 sampel data (v)
3. Akurasi pada training set dan validation set di atas 85%. (x)

5 Star Criteria
1. Semua ketentuan terpenuhi (?)
2. Dataset memiliki 3 kelas atau lebih (v)
3. Minimal 2000 sampel data (v)
4. Serta akurasi pada training set dan validation set di atas 90%. (x)

## Pre Processing
"""

# The process to acces Kaggle API refer to this article
# https://www.kaggle.com/general/74235
# Install kaggle API
! pip install -q kaggle

# Insert my kaggle json key
from google.colab import files
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

# # Download Google QUEST Q&A Labeling Competition Dataset
# # https://www.kaggle.com/c/google-quest-challenge/data
! kaggle competitions download -c google-quest-challenge

# Unzip the dataset
! unzip /content/train.csv.zip

# Import libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras import regularizers
from keras.regularizers import l2

# Read train csv file
df = pd.read_csv('/content/train.csv')
df.head()

# Gain insights
df.info()

# Check Null Value
df.isnull().sum()

# Read csv file
df_2 = pd.read_csv('/content/test.csv')
df_2.head()

# Gain insights
df_2.info()

# Check Null Value
df_2.isnull().sum()

# Pre processing train.csv
category_1 = pd.get_dummies(df.category)
new_df_1 = pd.concat([df.question_title, category_1], axis=1)
new_df_1

# Pre processing test.csv
category_2 = pd.get_dummies(df_2.category)
new_df_2 = pd.concat([df_2.question_title, category_2], axis=1)
new_df_2

# Append test.csv into train,csv
latest_df = new_df_1.append(new_df_2, ignore_index=True)
latest_df

# Gain insight from latest_df
latest_df.info()

# Check imbalance data
culture = latest_df['CULTURE'].value_counts()
life_art = latest_df['LIFE_ARTS'].value_counts()
science = latest_df['SCIENCE'].value_counts()
stack = latest_df['STACKOVERFLOW'].value_counts()
tech = latest_df['TECHNOLOGY'].value_counts()
print(culture, life_art, science, stack, tech)

# Gain insight from latest_df
latest_df.info()

# Make Clean Function to lowering strings and add space between punctuations
# Inspired by this article
# https://www.kaggle.com/hamishdickson/using-keras-oov-tokens
def clean_str(string):
    string = str(string)
    string = string.lower()
    
    # punctuations = '!"#$%&()*+,-./:;<=>@[\]^_`{|}~ '

    specials = [',', '?', '!', '$']
    for s in specials:
        string = string.replace(s, f' {s} ')
        
    return string

# Test our clean function
test_string = "How do you make a binary image in Photoshop?"
test_string = clean_str(test_string)
print(test_string)

# Convert Dataframe into numpy array
headline = latest_df['question_title'].apply(clean_str)
headline = headline.values
label = latest_df[[
                'CULTURE',
                'LIFE_ARTS',
                'SCIENCE',
                'STACKOVERFLOW',
                'TECHNOLOGY',
]].values

# Split data into train and test data
train_headline, test_headline, train_label, test_label = train_test_split(headline, label, test_size=0.2)

# Tokenizer and tranform headline into sequences
tokenizer = Tokenizer(num_words=1000, oov_token='x')
tokenizer.fit_on_texts(train_headline) 
tokenizer.fit_on_texts(test_headline)
 
train_sequences = tokenizer.texts_to_sequences(train_headline)
test_sequences = tokenizer.texts_to_sequences(test_headline)
 
train_padded = pad_sequences(train_sequences) 
test_padded = pad_sequences(test_sequences)

our_sent = tokenizer.texts_to_sequences([test_string])
seq_sent = tokenizer.sequences_to_texts(our_sent)
print(our_sent, seq_sent)

"""## Build Machine Learning Model"""

# Build our Machine Learning Model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=1000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(512,
                          activation='relu',
                          kernel_regularizer=regularizers.l2(0.01),
                          bias_regularizer=regularizers.l2(1e-04)),
    tf.keras.layers.Dropout(rate=0.5),
    tf.keras.layers.Dense(256,
                          activation='relu',
                          kernel_regularizer=regularizers.l2(0.01),
                          bias_regularizer=regularizers.l2(1e-04)),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

# See model summary
model.summary()

# Build Callback Class
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9):
      if(logs.get('val_accuracy')>0.9):
        print("\nModel accuracy and Validation accuracy reach >90%!")
        self.model.stop_training = True
callbacks = myCallback()

"""## Train and Test Model"""

# Test our Machine Learning Model
num_epochs = 500
history = model.fit(train_padded,
                    train_label,
                    epochs=num_epochs, 
                    validation_data=(test_padded, test_label),
                    callbacks=[callbacks],
                    verbose=2)

"""## Evaluate"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()